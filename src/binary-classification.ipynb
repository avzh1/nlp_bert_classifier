{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Binary Classification of the \"Don't Patronize Me!\" Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Perform Binary Classification to predict whether a text contains patronizing and condescending language. The task was task 4 (subtask 1) in the SemEval 2022 competition."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initialize Environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Library imports\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from nltk import PorterStemmer, WordNetLemmatizer\n",
    "import codecs\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import DataLoader, Dataset, Sampler\n",
    "\n",
    "import re\n",
    "\n",
    "# import torch.nn as nn\n",
    "# import torch.optim as optim\n",
    "# import torch.nn.functional as F\n",
    "\n",
    "def fix_seed(seed=420.69):\n",
    "  torch.manual_seed(seed)\n",
    "  torch.cuda.manual_seed(seed)\n",
    "#   np.random.seed(seed)\n",
    "#   random.seed(seed)\n",
    "\n",
    "fix_seed()\n",
    "\n",
    "data_path = '../data'\n",
    "embeddings_path = '../word_embeddings'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda:0\n"
     ]
    }
   ],
   "source": [
    "if not torch.cuda.is_available():\n",
    "  print('WARNING: You may want to change the runtime to GPU for faster training!')\n",
    "  DEVICE = 'cpu'\n",
    "else:\n",
    "  DEVICE = 'cuda:0'\n",
    "\n",
    "print(DEVICE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "h_params = {\n",
    "    'model': 'None',\n",
    "    'batch_size': 128\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "UsageError: Line magic function `%%bash` not found.\n"
     ]
    }
   ],
   "source": [
    "# Download the pre-trained model for word_embeddings\n",
    "# This part is similar to the tutorial\n",
    "\n",
    "# !wget http://nlp.stanford.edu/data/glove.6B.zip\n",
    "# !unzip 1/glove.6B.zip"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading\n",
    "\n",
    "Load the data into pandas dataframes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import Data\n",
    "train_data_path = f'{data_path}/dontpatronizeme_pcl.tsv'\n",
    "test_data_path  = f'{data_path}/task4_test.tsv'\n",
    "\n",
    "train_data = pd.read_csv(train_data_path, delimiter='\\t', skiprows=4, header=None, names=['par_id','art_id','keyword','country_code', 'text','label'])\n",
    "test_data  = pd.read_csv(test_data_path,  delimiter='\\t', skiprows=4, header=None, names=['par_id','art_id','keyword','country_code', 'text'])\n",
    "\n",
    "train_data = train_data.drop(['art_id'], axis=1)\n",
    "test_data = test_data.drop(['art_id'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Concatenate label information to train data\n",
    "dev_label_path   = f'{data_path}/dev_semeval_parids-labels.csv'\n",
    "train_label_path = f'{data_path}/train_semeval_parids-labels.csv'\n",
    "\n",
    "dev_label   = pd.read_csv(dev_label_path, delimiter=',')\n",
    "train_label = pd.read_csv(train_label_path, delimiter=',')\n",
    "\n",
    "detailed_labels = pd.concat([dev_label, train_label], ignore_index=True, join='inner', names=['simple', 'detailed'])\n",
    "train_data = pd.merge(train_data, detailed_labels, on='par_id')\n",
    "train_data = train_data.rename(columns={'label_x': 'label', 'label_y': 'label_detailed'})\n",
    "\n",
    "train_data = train_data.drop('par_id', axis=1)\n",
    "test_data = test_data.drop('par_id', axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add Binary Classification column for ease of checking\n",
    "train_data.loc[:, 'is_patronizing'] = False\n",
    "train_data.loc[train_data['label'].isin([2,3,4]), 'is_patronizing'] = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = train_data.dropna(subset = 'text')\n",
    "test_data = test_data.dropna(subset = 'text')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### DataLoaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "tuple indices must be integers or slices, not str",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[45], line 61\u001b[0m\n\u001b[1;32m     58\u001b[0m custom_dataset \u001b[38;5;241m=\u001b[39m TrainDataset(train_data)\u001b[38;5;241m.\u001b[39mapply_lemmatization()\n\u001b[1;32m     59\u001b[0m train_loader \u001b[38;5;241m=\u001b[39m DataLoader(dataset\u001b[38;5;241m=\u001b[39mcustom_dataset, batch_size\u001b[38;5;241m=\u001b[39mh_params[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mbatch_size\u001b[39m\u001b[38;5;124m'\u001b[39m], shuffle\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, collate_fn\u001b[38;5;241m=\u001b[39mcustom_dataset\u001b[38;5;241m.\u001b[39mcollate_fn)\n\u001b[0;32m---> 61\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m test, label \u001b[38;5;129;01min\u001b[39;00m train_loader:\n\u001b[1;32m     62\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;28mlen\u001b[39m(test))\n\u001b[1;32m     63\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;28mlen\u001b[39m(label))\n",
      "File \u001b[0;32m~/Documents/imperial/year4/lectures/.venv/lib/python3.10/site-packages/torch/utils/data/dataloader.py:631\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    628\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    629\u001b[0m     \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[1;32m    630\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[0;32m--> 631\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_next_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    632\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m    633\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_kind \u001b[38;5;241m==\u001b[39m _DatasetKind\u001b[38;5;241m.\u001b[39mIterable \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[1;32m    634\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[1;32m    635\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called:\n",
      "File \u001b[0;32m~/Documents/imperial/year4/lectures/.venv/lib/python3.10/site-packages/torch/utils/data/dataloader.py:675\u001b[0m, in \u001b[0;36m_SingleProcessDataLoaderIter._next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    673\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_next_data\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    674\u001b[0m     index \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_next_index()  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[0;32m--> 675\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_dataset_fetcher\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfetch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mindex\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[1;32m    676\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory:\n\u001b[1;32m    677\u001b[0m         data \u001b[38;5;241m=\u001b[39m _utils\u001b[38;5;241m.\u001b[39mpin_memory\u001b[38;5;241m.\u001b[39mpin_memory(data, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory_device)\n",
      "File \u001b[0;32m~/Documents/imperial/year4/lectures/.venv/lib/python3.10/site-packages/torch/utils/data/_utils/fetch.py:54\u001b[0m, in \u001b[0;36m_MapDatasetFetcher.fetch\u001b[0;34m(self, possibly_batched_index)\u001b[0m\n\u001b[1;32m     52\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     53\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[possibly_batched_index]\n\u001b[0;32m---> 54\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcollate_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[45], line 45\u001b[0m, in \u001b[0;36mTrainDataset.collate_fn\u001b[0;34m(data_batch, label_batch)\u001b[0m\n\u001b[1;32m     42\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mnn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mrnn\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m pad_sequence\n\u001b[1;32m     44\u001b[0m \u001b[38;5;66;03m# Pad text sequences in the batch\u001b[39;00m\n\u001b[0;32m---> 45\u001b[0m padded_text \u001b[38;5;241m=\u001b[39m pad_sequence([torch\u001b[38;5;241m.\u001b[39mtensor(data[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtext\u001b[39m\u001b[38;5;124m'\u001b[39m]) \u001b[38;5;28;01mfor\u001b[39;00m data \u001b[38;5;129;01min\u001b[39;00m data_batch], batch_first\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, padding_value\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m)\n\u001b[1;32m     47\u001b[0m data_batch_padded \u001b[38;5;241m=\u001b[39m {\n\u001b[1;32m     48\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mkeyword\u001b[39m\u001b[38;5;124m'\u001b[39m: torch\u001b[38;5;241m.\u001b[39mtensor([data[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mkeyword\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;28;01mfor\u001b[39;00m data \u001b[38;5;129;01min\u001b[39;00m data_batch]),\n\u001b[1;32m     49\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcountry_code\u001b[39m\u001b[38;5;124m'\u001b[39m: torch\u001b[38;5;241m.\u001b[39mtensor([data[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcountry_code\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;28;01mfor\u001b[39;00m data \u001b[38;5;129;01min\u001b[39;00m data_batch]),\n\u001b[1;32m     50\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtext\u001b[39m\u001b[38;5;124m'\u001b[39m: padded_text\n\u001b[1;32m     51\u001b[0m }\n\u001b[1;32m     53\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m data_batch_padded, label_batch\n",
      "Cell \u001b[0;32mIn[45], line 45\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     42\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mnn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mrnn\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m pad_sequence\n\u001b[1;32m     44\u001b[0m \u001b[38;5;66;03m# Pad text sequences in the batch\u001b[39;00m\n\u001b[0;32m---> 45\u001b[0m padded_text \u001b[38;5;241m=\u001b[39m pad_sequence([torch\u001b[38;5;241m.\u001b[39mtensor(\u001b[43mdata\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mtext\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m) \u001b[38;5;28;01mfor\u001b[39;00m data \u001b[38;5;129;01min\u001b[39;00m data_batch], batch_first\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, padding_value\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m)\n\u001b[1;32m     47\u001b[0m data_batch_padded \u001b[38;5;241m=\u001b[39m {\n\u001b[1;32m     48\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mkeyword\u001b[39m\u001b[38;5;124m'\u001b[39m: torch\u001b[38;5;241m.\u001b[39mtensor([data[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mkeyword\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;28;01mfor\u001b[39;00m data \u001b[38;5;129;01min\u001b[39;00m data_batch]),\n\u001b[1;32m     49\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcountry_code\u001b[39m\u001b[38;5;124m'\u001b[39m: torch\u001b[38;5;241m.\u001b[39mtensor([data[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcountry_code\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;28;01mfor\u001b[39;00m data \u001b[38;5;129;01min\u001b[39;00m data_batch]),\n\u001b[1;32m     50\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtext\u001b[39m\u001b[38;5;124m'\u001b[39m: padded_text\n\u001b[1;32m     51\u001b[0m }\n\u001b[1;32m     53\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m data_batch_padded, label_batch\n",
      "\u001b[0;31mTypeError\u001b[0m: tuple indices must be integers or slices, not str"
     ]
    }
   ],
   "source": [
    "class TrainDataset(Dataset):\n",
    "    def __init__(self, dataframe):\n",
    "        self.data = dataframe\n",
    "\n",
    "        self.stemming = False\n",
    "        self.lemmatization = False\n",
    "        self.word_embeddings = False\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "\n",
    "        text = list(filter(None, re.split('[\\s,/.\\']', self.data['text'].iloc[idx])))\n",
    "\n",
    "        if self.stemming:           \n",
    "            porter = PorterStemmer()\n",
    "            text = list(map(porter.stem, text))\n",
    "\n",
    "        if self.lemmatization:\n",
    "            wordnet_lemmatizer = WordNetLemmatizer()\n",
    "            text = list(map(wordnet_lemmatizer.lemmatize, text))\n",
    "\n",
    "        if self.word_embeddings:\n",
    "            text = apply_word_embeddings(text)\n",
    "\n",
    "        data = {'keyword'     : self.data['keyword'].iloc[idx],\n",
    "                'country_code': self.data['country_code'].iloc[idx],\n",
    "                'text'        : text}\n",
    "        \n",
    "        label = {'label'        : self.data['label'].iloc[idx],\n",
    "                'label_detailed': self.data['label_detailed'].iloc[idx],\n",
    "                'is_patronizing': self.data['is_patronizing'].iloc[idx]}\n",
    "\n",
    "        return data, label\n",
    "    \n",
    "    def enable_stemming(self):\n",
    "        self.stemming = True\n",
    "        return self\n",
    "    \n",
    "    def enable_lemmatization(self):\n",
    "        self.lemmatization = True\n",
    "        return self\n",
    "    \n",
    "    def enable_word_embeddings(self):\n",
    "        self.word_embeddings = True\n",
    "        return self\n",
    "\n",
    "    def apply_word_embeddings(text: str):\n",
    "        w2i = [] # word2index\n",
    "        i2w = [] # index2word\n",
    "        wvecs = [] # word vectors\n",
    "\n",
    "        # this is a large file, it will take a while to load in the memory!\n",
    "        with codecs.open(f'{embeddings_path}/glove.6B.50d.txt', 'r','utf-8') as f:\n",
    "            index = 0\n",
    "            for line in tqdm(f.readlines()):\n",
    "                # Ignore the first line - first line typically contains vocab, dimensionality\n",
    "                if len(line.strip().split()) > 3:\n",
    "\n",
    "                (word, vec) = (line.strip().split()[0],\n",
    "                                list(map(float,line.strip().split()[1:])))\n",
    "\n",
    "                wvecs.append(vec)\n",
    "                w2i.append((word, index))\n",
    "                i2w.append((index, word))\n",
    "                index += 1\n",
    "\n",
    "        w2i = dict(w2i)\n",
    "        i2w = dict(i2w)\n",
    "        wvecs = np.array(wvecs)\n",
    "\n",
    "        return None\n",
    "\n",
    "    def collate_fn(data_batch, label_batch):\n",
    "        from torch.nn.utils.rnn import pad_sequence\n",
    "\n",
    "        # Pad text sequences in the batch\n",
    "        padded_text = pad_sequence([torch.tensor(data['text']) for data in data_batch], batch_first=True, padding_value=0)\n",
    "\n",
    "        data_batch_padded = {\n",
    "            'keyword': torch.tensor([data['keyword'] for data in data_batch]),\n",
    "            'country_code': torch.tensor([data['country_code'] for data in data_batch]),\n",
    "            'text': padded_text\n",
    "        }\n",
    "\n",
    "        return data_batch_padded, label_batch\n",
    "\n",
    "        \n",
    "\n",
    "# Assuming train_data is your DataFrame\n",
    "custom_dataset = TrainDataset(train_data).enable_lemmatization()\n",
    "train_loader = DataLoader(dataset=custom_dataset, batch_size=h_params['batch_size'], shuffle=True, collate_fn=custom_dataset.collate_fn)\n",
    "\n",
    "for test, label in train_loader:\n",
    "    print(len(test))\n",
    "    print(len(label))\n",
    "    print(test['text'])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
