{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Binary Classification of the \"Don't Patronize Me!\" Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Perform Binary Classification to predict whether a text contains patronizing and condescending language. The task was task 4 (subtask 1) in the SemEval 2022 competition."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initialize Environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Library imports\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from nltk import PorterStemmer, WordNetLemmatizer\n",
    "import codecs\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import DataLoader, Dataset, Sampler\n",
    "from tqdm import tqdm\n",
    "import torch.nn as nn\n",
    "\n",
    "import re\n",
    "\n",
    "import numpy as np\n",
    "from numpy import dot\n",
    "from numpy.linalg import norm\n",
    "\n",
    "\n",
    "\n",
    "# import torch.optim as optim\n",
    "# import torch.nn.functional as F\n",
    "\n",
    "def fix_seed(seed=420.69):\n",
    "  torch.manual_seed(seed)\n",
    "  torch.cuda.manual_seed(seed)\n",
    "#   np.random.seed(seed)\n",
    "#   random.seed(seed)\n",
    "\n",
    "fix_seed()\n",
    "\n",
    "data_path = '../data'\n",
    "embeddings_path = '../word_embeddings'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda:0\n"
     ]
    }
   ],
   "source": [
    "if not torch.cuda.is_available():\n",
    "  print('WARNING: You may want to change the runtime to GPU for faster training!')\n",
    "  DEVICE = 'cpu'\n",
    "else:\n",
    "  DEVICE = 'cuda:0'\n",
    "\n",
    "print(DEVICE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "h_params = {\n",
    "    'model': 'None'\n",
    "    , 'batch_size': 128\n",
    "    , 'embedding_dimensions': 50\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Raw Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading\n",
    "\n",
    "Load the data into pandas dataframes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import Data\n",
    "train_data_path = f'{data_path}/dontpatronizeme_pcl.tsv'\n",
    "test_data_path  = f'{data_path}/task4_test.tsv'\n",
    "\n",
    "train_data = pd.read_csv(train_data_path, delimiter='\\t', skiprows=4, header=None, names=['par_id','art_id','keyword','country_code', 'text','label'])\n",
    "test_data  = pd.read_csv(test_data_path,  delimiter='\\t', skiprows=4, header=None, names=['par_id','art_id','keyword','country_code', 'text'])\n",
    "\n",
    "train_data = train_data.drop(['art_id'], axis=1)\n",
    "test_data = test_data.drop(['art_id'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Concatenate label information to train data\n",
    "dev_label_path   = f'{data_path}/dev_semeval_parids-labels.csv'\n",
    "train_label_path = f'{data_path}/train_semeval_parids-labels.csv'\n",
    "\n",
    "dev_label   = pd.read_csv(dev_label_path, delimiter=',')\n",
    "train_label = pd.read_csv(train_label_path, delimiter=',')\n",
    "\n",
    "detailed_labels = pd.concat([dev_label, train_label], ignore_index=True, join='inner', names=['simple', 'detailed'])\n",
    "train_data = pd.merge(train_data, detailed_labels, on='par_id')\n",
    "train_data = train_data.rename(columns={'label_x': 'label', 'label_y': 'label_detailed'})\n",
    "\n",
    "train_data = train_data.drop('par_id', axis=1)\n",
    "test_data = test_data.drop('par_id', axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add Binary Classification column for ease of checking\n",
    "train_data.loc[:, 'is_patronizing'] = False\n",
    "train_data.loc[train_data['label'].isin([2,3,4]), 'is_patronizing'] = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = train_data.dropna(subset = 'text')\n",
    "test_data = test_data.dropna(subset = 'text')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pre-Processing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Standardize Input\n",
    "\n",
    "The idea is, that we have variable sentence lengths. For implementation of the networks coming up ahead, we need tensors to be of constant dimensions. Therefore, we must pad sentences which are too short. Therefore, we provide an alternative implementation of DataLoaders that also pre-process the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Tokenize input\n",
    "def get_tokenized_corpus(corpus):\n",
    "  tokenized_corpus = []\n",
    "\n",
    "  for sentence in corpus:\n",
    "    tokenized_sentence = []\n",
    "    for token in sentence.split(' '):\n",
    "      tokenized_sentence.append(token)\n",
    "    tokenized_corpus.append(tokenized_sentence)\n",
    "\n",
    "  return tokenized_corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. Create mappings from word to index\n",
    "def get_word2idx(tokenized_corpus):\n",
    "  vocabulary = []\n",
    "  for sentence in tokenized_corpus:\n",
    "    for token in sentence:\n",
    "        if token not in vocabulary:\n",
    "            vocabulary.append(token)\n",
    "\n",
    "  word2idx = {w: idx+2 for (idx, w) in enumerate(vocabulary)}\n",
    "  word2idx['<pad>'] = 0      # padding tokens assigned to index 0.\n",
    "  word2idx['<unknown>'] = 1  # unknown tokens are assigned to index 1. (we can, during training, mask some of the inputs to <unknown> so the model knows what to do with these tokens also)\n",
    "\n",
    "  return word2idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PreProcessor(Dataset):\n",
    "    def __init__(self, chain_of_transformations):\n",
    "        \"\"\"PreProcessor which takes a list of functions which are chainable in the order provided that will transform the data.\"\"\"\n",
    "        self.chain_of_transformations = chain_of_transformations\n",
    "\n",
    "    def \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LabelledDataset(Dataset):\n",
    "    def __init__(self, dataframe):\n",
    "        self.data = dataframe\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "\n",
    "        data = {'keyword'     : self.data['keyword'].iloc[idx],\n",
    "                'country_code': self.data['country_code'].iloc[idx],\n",
    "                'text'        : self.data['text'].iloc[idx]}\n",
    "        \n",
    "        label = {'label'        : self.data['label'].iloc[idx],\n",
    "                'label_detailed': self.data['label_detailed'].iloc[idx],\n",
    "                'is_patronizing': self.data['is_patronizing'].iloc[idx]}\n",
    "\n",
    "        return data, label\n",
    "    \n",
    "class WithheldDataset(Dataset):\n",
    "    def __init__(self, dataframe):\n",
    "        self.data = dataframe\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        data = {'keyword'     : self.data['keyword'].iloc[idx],\n",
    "                'country_code': self.data['country_code'].iloc[idx],\n",
    "                'text'        : self.data['text'].iloc[idx]}\n",
    "        \n",
    "        return data\n",
    "\n",
    "train_loader = DataLoader(dataset=LabelledDataset(train_data), batch_size=h_params['batch_size'], shuffle=True)\n",
    "test_loader = DataLoader(dataset=WithheldDataset(train_data), batch_size=h_params['batch_size'], shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pre-trained embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define Embedding Class\n",
    "class Encoder(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_dimension):\n",
    "        super(Encoder, self)\n",
    "        # create embedding object to store mappings from words to embedded vectors\n",
    "        self.tokenEmbeddings = nn.Embedding(num_embeddings=vocab_size\n",
    "                                          , embedding_dim=embedding_dimension\n",
    "                                          , padding_idx=0)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        \"\"\"Generates the embeddings for each word in the input, of size: (batch_size, max_length of training sample)\"\"\"\n",
    "        embedded = self.embedding(x)\n",
    "        embedded = embedded.unsqueeze(1)\n",
    "\n",
    "        return embedded"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download the pre-trained model for word_embeddings\n",
    "# This part is similar to the tutorial\n",
    "\n",
    "# !wget http://nlp.stanford.edu/data/glove.6B.zip\n",
    "# !unzip 1/glove.6B.zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Pre-trained Embeddings from GloVe\n",
    "\n",
    "word2idx = \n",
    "\n",
    "w2i = [] # word2index\n",
    "i2w = [] # index2word\n",
    "wvecs = [] # word vectors\n",
    "\n",
    "# this is a large file, it will take a while to load in the memory!\n",
    "with codecs.open(f'glove.6B.{h_params['embedding_dimensions']}d.txt', 'r','utf-8') as f:\n",
    "  index = 0\n",
    "  for line in tqdm(f.readlines()):\n",
    "    # Ignore the first line - first line typically contains vocab, dimensionality\n",
    "    if len(line.strip().split()) > 3:\n",
    "\n",
    "      (word, vec) = (line.strip().split()[0],\n",
    "                     list(map(float,line.strip().split()[1:])))\n",
    "\n",
    "      wvecs.append(vec)\n",
    "      w2i.append((word, index))\n",
    "      i2w.append((index, word))\n",
    "      index += 1\n",
    "\n",
    "w2i = dict(w2i)\n",
    "i2w = dict(i2w)\n",
    "wvecs = np.array(wvecs)\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
