{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Library imports\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from nltk import PorterStemmer, WordNetLemmatizer\n",
    "import codecs\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import DataLoader, Dataset, Sampler\n",
    "from tqdm import tqdm\n",
    "import torch.nn as nn\n",
    "\n",
    "import re\n",
    "\n",
    "import numpy as np\n",
    "from numpy import dot\n",
    "from numpy.linalg import norm\n",
    "\n",
    "\n",
    "\n",
    "# import torch.optim as optim\n",
    "# import torch.nn.functional as F\n",
    "\n",
    "def fix_seed(seed=420.69):\n",
    "  torch.manual_seed(seed)\n",
    "  torch.cuda.manual_seed(seed)\n",
    "#   np.random.seed(seed)\n",
    "#   random.seed(seed)\n",
    "\n",
    "fix_seed()\n",
    "\n",
    "data_path = '../data'\n",
    "embeddings_path = '../word_embeddings'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda:0\n"
     ]
    }
   ],
   "source": [
    "if not torch.cuda.is_available():\n",
    "  print('WARNING: You may want to change the runtime to GPU for faster training!')\n",
    "  DEVICE = 'cpu'\n",
    "else:\n",
    "  DEVICE = 'cuda:0'\n",
    "\n",
    "print(DEVICE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Importing the Data\n",
    "\n",
    "(taken form binary-classification.ipynb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import Data\n",
    "train_data_path = f'{data_path}/dontpatronizeme_pcl.tsv'\n",
    "test_data_path  = f'{data_path}/task4_test.tsv'\n",
    "\n",
    "train_data = pd.read_csv(train_data_path, delimiter='\\t', skiprows=4, header=None, names=['par_id','art_id','keyword','country_code', 'text','label'])\n",
    "test_data  = pd.read_csv(test_data_path,  delimiter='\\t', skiprows=4, header=None, names=['par_id','art_id','keyword','country_code', 'text'])\n",
    "\n",
    "train_data = train_data.drop(['art_id'], axis=1)\n",
    "test_data = test_data.drop(['art_id'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Concatenate label information to train data\n",
    "dev_label_path   = f'{data_path}/dev_semeval_parids-labels.csv'\n",
    "train_label_path = f'{data_path}/train_semeval_parids-labels.csv'\n",
    "\n",
    "dev_label   = pd.read_csv(dev_label_path, delimiter=',')\n",
    "train_label = pd.read_csv(train_label_path, delimiter=',')\n",
    "\n",
    "detailed_labels = pd.concat([dev_label, train_label], ignore_index=True, join='inner', names=['simple', 'detailed'])\n",
    "train_data = pd.merge(train_data, detailed_labels, on='par_id')\n",
    "train_data = train_data.rename(columns={'label_x': 'label', 'label_y': 'label_detailed'})\n",
    "\n",
    "train_data = train_data.drop('par_id', axis=1)\n",
    "test_data = test_data.drop('par_id', axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add Binary Classification column for ease of checking\n",
    "train_data.loc[:, 'is_patronizing'] = False\n",
    "train_data.loc[train_data['label'].isin([2,3,4]), 'is_patronizing'] = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = train_data.dropna(subset = 'text')\n",
    "test_data = test_data.dropna(subset = 'text')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pre-processing ideas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Synonym word replacement\n",
    "\n",
    "- first article: https://towardsdatascience.com/data-preprocessing-in-nlp-c371d53ba3e0\n",
    "- how to get list of synonyms: https://stackoverflow.com/questions/19258652/how-to-get-synonyms-from-nltk-wordnet-python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['do-or-die', 'heroic', 'dire', 'desperate', 'despairing']"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# For getting a list of synonyms for a word\n",
    "\n",
    "from itertools import chain\n",
    "from nltk.corpus import wordnet\n",
    "\n",
    "source_word_for_synonym = 'desperate'\n",
    "\n",
    "synonyms = wordnet.synsets(source_word_for_synonym)\n",
    "lemmas = set(chain.from_iterable([word.lemma_names() for word in synonyms])) # returns a sorted list of synonyms (including the source word also)\n",
    "\n",
    "lemmas_list = list(lemmas)\n",
    "lemmas_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1. 0. 0. 0. 0.]\n",
      " [0. 1. 0. 0. 0.]\n",
      " [0. 0. 1. 0. 0.]\n",
      " [0. 0. 0. 1. 0.]\n",
      " [0. 0. 0. 0. 1.]]\n",
      "[0. 0. 0. 0.]\n",
      "The most similar word to 'desperate' is 'do-or-die' with a cosine similarity of 0.00\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "# Calculate cosine similarity\n",
    "vectorizer = CountVectorizer().fit_transform(lemmas_list)\n",
    "cosine_similarities = cosine_similarity(vectorizer)\n",
    "\n",
    "print(cosine_similarities)\n",
    "\n",
    "print(cosine_similarities[0, 1:])\n",
    "\n",
    "# Find the index of the word with the highest similarity (excluding the source word itself)\n",
    "most_similar_index = cosine_similarities[0, 1:].argmax()\n",
    "\n",
    "# Get the most similar word\n",
    "most_similar_word = lemmas_list[most_similar_index]\n",
    "\n",
    "print(f\"The most similar word to '{source_word_for_synonym}' is '{most_similar_word}' with a cosine similarity of {cosine_similarities[0, most_similar_index + 1]:.2f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Example from https://stackoverflow.com/questions/73791396/python-cosine-similarity-between-sentences-with-synonyms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(0.0, 'He belongs to USA')\n",
      "(0.1889822365046136, 'Hindustan is synonym of my country name')\n",
      "(0.0, 'USA and America is same')\n",
      "(0.4082482904638631, 'You live in a great country.')\n",
      "(0.20412414523193154, 'All countries are great to live')\n"
     ]
    }
   ],
   "source": [
    "import math\n",
    "import re\n",
    "from collections import Counter\n",
    "\n",
    "WORD = re.compile(r\"\\w+\")    \n",
    "def get_cosine(vec1, vec2):\n",
    "    intersection = set(vec1.keys()) & set(vec2.keys())\n",
    "    numerator = sum([vec1[x] * vec2[x] for x in intersection])\n",
    "\n",
    "    sum1 = sum([vec1[x] ** 2 for x in list(vec1.keys())])\n",
    "    sum2 = sum([vec2[x] ** 2 for x in list(vec2.keys())])\n",
    "    denominator = math.sqrt(sum1) * math.sqrt(sum2)\n",
    "\n",
    "    if not denominator:\n",
    "        return 0.0\n",
    "    else:\n",
    "        return float(numerator) / denominator    \n",
    "def text_to_vector(text):\n",
    "    words = WORD.findall(text)\n",
    "    return Counter(words)\n",
    "\n",
    "synonyms = {\"India\": \"Hindustan\",\n",
    "            \"USA\": \"America\",}    \n",
    "\n",
    "def map_synon(text):\n",
    "    return ' '.join([w if not w in synonyms  else synonyms[w] for w in text.split(' ')])\n",
    "text2 = \"I live in India\"\n",
    "text2 = map_synon(text2)\n",
    "\n",
    "sentences = [\"He belongs to USA\", \n",
    "            \"Hindustan is synonym of my country name\",\n",
    "            \"USA and America is same\",\n",
    "            \"You live in a great country.\",\n",
    "            \"All countries are great to live\",]    \n",
    "cosinetolist = []\n",
    "\n",
    "\n",
    "for i in sentences:\n",
    "    vector1 = text_to_vector(map_synon(i))\n",
    "    vector2 = text_to_vector(text2) \n",
    "    cosine = get_cosine(vector1, vector2)\n",
    "    cosinetolist.append((cosine,i,))\n",
    "\n",
    "l = cosinetolist\n",
    "\n",
    "for r in l:\n",
    "    print(r)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From https://www.machinelearningplus.com/nlp/cosine-similarity/?utm_content=cmp-true"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'CountVectorizer' object has no attribute 'get_feature_names'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[84], line 22\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[38;5;66;03m# OPTIONAL: Convert Sparse Matrix to Pandas Dataframe if you want to see the word frequencies.\u001b[39;00m\n\u001b[1;32m     20\u001b[0m doc_term_matrix \u001b[38;5;241m=\u001b[39m sparse_matrix\u001b[38;5;241m.\u001b[39mtodense()\n\u001b[1;32m     21\u001b[0m df \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mDataFrame(doc_term_matrix, \n\u001b[0;32m---> 22\u001b[0m                   columns\u001b[38;5;241m=\u001b[39m\u001b[43mcount_vectorizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_feature_names\u001b[49m(), \n\u001b[1;32m     23\u001b[0m                   index\u001b[38;5;241m=\u001b[39m[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdoc_trump\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdoc_election\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdoc_putin\u001b[39m\u001b[38;5;124m'\u001b[39m])\n\u001b[1;32m     24\u001b[0m df\n\u001b[1;32m     26\u001b[0m \u001b[38;5;66;03m# Compute Cosine Similarity\u001b[39;00m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'CountVectorizer' object has no attribute 'get_feature_names'"
     ]
    }
   ],
   "source": [
    "# Define the documents\n",
    "doc_trump = \"Mr. Trump became president after winning the political election. Though he lost the support of some republican friends, Trump is friends with President Putin\"\n",
    "\n",
    "doc_election = \"President Trump says Putin had no political interference is the election outcome. He says it was a witchhunt by political parties. He claimed President Putin is a friend who had nothing to do with the election\"\n",
    "\n",
    "doc_putin = \"Post elections, Vladimir Putin became President of Russia. President Putin had served as the Prime Minister earlier in his political career\"\n",
    "\n",
    "documents = [doc_trump, doc_election, doc_putin]\n",
    "\n",
    "# Scikit Learn\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "import pandas as pd\n",
    "\n",
    "# Create the Document Term Matrix\n",
    "count_vectorizer = CountVectorizer(stop_words='english')\n",
    "count_vectorizer = CountVectorizer()\n",
    "sparse_matrix = count_vectorizer.fit_transform(documents)\n",
    "\n",
    "# OPTIONAL: Convert Sparse Matrix to Pandas Dataframe if you want to see the word frequencies.\n",
    "doc_term_matrix = sparse_matrix.todense()\n",
    "df = pd.DataFrame(doc_term_matrix, \n",
    "                  columns=count_vectorizer.get_feature_names(), \n",
    "                  index=['doc_trump', 'doc_election', 'doc_putin'])\n",
    "df\n",
    "\n",
    "# Compute Cosine Similarity\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "print(cosine_similarity(df, df))\n",
    "#> [[ 1.          0.48927489  0.37139068]\n",
    "#>  [ 0.48927489  1.          0.38829014]\n",
    "#>  [ 0.37139068  0.38829014  1.        ]]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
